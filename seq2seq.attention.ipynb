{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelimineries\n",
    "\n",
    "This code require PyTorch >= 0.4.1, and was tested with Python 3.6. Make sure that you have installed the Numpy package as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here we first import the needed packages.\n",
    "The seed is used for reproducibility, different seeds may \n",
    "lead to different results. \n",
    "'''\n",
    "!pip install torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model class\n",
    "\n",
    "Our Sequence to sequence model is based on the encoder-decoder architecture. Here we use GRUs as the neural networks. The decoder is initialized by the final states of the encoder (encoder_final below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows the generator class, it simply projects the pre-output layer to obtain the output layer, so that the final dimension is the target vocabulary size. (embedding size --> target vocabulary size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "We use a bi-directional GRU as our encoder. \n",
    "\n",
    "We use mini-batch training which process instances parallelly. However, the sentences in a mini-batch may have different sentence lengths. We need to add paddings to the shorter sentences. Luckily, PyTorch has defined functions to deal with these padding tokens during training. \n",
    "\n",
    "The encoder reads in a source sentence (a sequence of word embeddings) and produces hidden states. It also returns a final vector which is a summary of the entire sentence, by concatenating the hidden state at the first token (backward) and the hidden state at the last token (forward). We will use this final vector to initialize the decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # we need to manually concatenate the final states for both directions\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
    "\n",
    "        return output, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "The decoder is a conditional GRU. The initial hidden state results from a projection of the encoder final vector.\n",
    "\n",
    "## Training\n",
    "In forward you can find a for-loop that computes the decoder hidden states one time step at a time. Note that, during training, we know exactly what the target words should be! We simply feed the correct previous target word embedding to the GRU at each time step. \n",
    "\n",
    "The forward function returns all decoder hidden states and pre-output vectors. Elsewhere these are used to compute the loss, after which the parameters are updated.\n",
    "\n",
    "## Prediction/Inference\n",
    "For prediction, forward function is only used for a single time step. After predicting a word from the returned pre-output vector, we can call it again, supplying it the word embedding of the previously predicted word and the last state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    #TODO: you need to add attention to the decoder initialzation function here\n",
    "    def __init__(self, emb_size, hidden_size, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "        #TODO: compute context vector using attention mechanism here\n",
    "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
    "        \n",
    "        \n",
    "        #TODO: update decoder rnn hidden state\n",
    "        \n",
    "        #TODO: cancatenate the three input in Equation 4 in Badhdanau's paper\n",
    "        # as the pre_output\n",
    "        \n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
    "                src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "                                         \n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "        \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "        \n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "We use attention mechanisms to extract features from Encoders dynamically at each decoding step. In this session, you need to implement an attention model based on Bahdanau's paper: Neural Machine Translation by Jointly Learning to Align and Translate. Section 3 and lecture slides (3-11/52) are the most important materials for the implementation. \n",
    "\n",
    "When you have finished your implementation, please try to replace the MLP function with a simpler dot-product to compute the scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.score_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.weights = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        #TODO: We first project the query (the decoder state).\n",
    "        \n",
    "        #TODO: Calculate scores using query and proj_key with an MLP.\n",
    "        \n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        #TODO: Turn scores to probabilities (weights).\n",
    "        \n",
    "        #TODO: Compute the context vector (weighted sum of the values).\n",
    "        \n",
    "        # return both context vector and the attention weights\n",
    "        # context shape: [B, 1, 2D], weights shape: [B, 1, M]\n",
    "        # B denotes the batch size, D means the dimension, M is maximum source length\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Softmax\n",
    "\n",
    "We use learned embeddings to convert the input tokens and output tokens to vectors of dimension emb_size. We will simply use PyTorch's nn.Embedding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model\n",
    "\n",
    "Here follows a function from hyperparameters to a full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    #TODO: create an attention function here \n",
    "    # and pass to the Decoder in the following code\n",
    "    \n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This section describes the training regime for our models.\n",
    "\n",
    "We first introduce some of the tools needed to train a standard encoder decoder model. We define a batch object that holds the src and target sentences for training, as well as their lengths and masks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The code below trains the model for 1 epoch (pass through the entire training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        \n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        \n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "We will use the Adam optimizer with default settings ($\\beta_1=0.9$, $\\beta_2=0.999$ and $\\epsilon=10^{-8}$).\n",
    "\n",
    "We will use $0.0003$ as the learning rate here, but for different problems another learning rate may be more appropriate. You will have to tune that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing examples\n",
    "\n",
    "To monitor progress during training, we will translate a few examples.\n",
    "\n",
    "We use greedy decoding for simplicity; that is, at each time step, starting at the first token, we choose the one with that maximum probability. (inferior to beam search decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    # cut off everything starting from </s> \n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n",
    "  \n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=3, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "      \n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        \n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We will load the dataset using torchtext. The default data are tokenized and are ready for training. You can change the data set by revising the path and the language pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
	"!pip install torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "\n",
    "if True:\n",
    "    \n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"    \n",
    "    SOS_TOKEN = \"<s>\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    LOWER = True\n",
    "    \n",
    "    # we include lengths to provide to the RNNs\n",
    "    SRC = data.Field(batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "    TRG = data.Field(batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "    MAX_LEN = 40  # NOTE: we filter out a lot of sentences for speed\n",
    "    \n",
    "    train_data = TranslationDataset(path=\"/home/staff/gongbo/teaching/data/sven.train\",\n",
    "                                    exts=(\".sv\", \".en\"),\n",
    "                                    fields=(SRC, TRG),\n",
    "                                    filter_pred=\n",
    "                                    lambda x: len(vars(x)['src'])\n",
    "                                    <= MAX_LEN\n",
    "                                    and len(vars(x)['trg'])<= MAX_LEN)\n",
    "    \n",
    "    valid_data = TranslationDataset(path=\"/home/staff/gongbo/teaching/data/sven.dev\",\n",
    "                                    exts=(\".sv\", \".en\"),\n",
    "                                    fields=(SRC, TRG),\n",
    "                                    filter_pred=\n",
    "                                    lambda x: len(vars(x)['src'])\n",
    "                                    <= MAX_LEN\n",
    "                                    and len(vars(x)['trg'])<= MAX_LEN)\n",
    "    test_data = TranslationDataset(path=\"/home/staff/gongbo/teaching/data/sven.test\",\n",
    "                                    exts=(\".sv\", \".en\"),\n",
    "                                    fields=(SRC, TRG),\n",
    "                                    filter_pred=\n",
    "                                    lambda x: len(vars(x)['src'])\n",
    "                                    <= MAX_LEN\n",
    "                                    and len(vars(x)['trg'])<= MAX_LEN)\n",
    "    \n",
    "    MIN_FREQ = 1  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "    SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "    TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    \n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterators\n",
    "\n",
    "Batching matters a ton for speed. We will use torch text's BucketIterator here to get batches containing sentences of (almost) the same length.\n",
    "\n",
    "#### Note on sorting batches for RNNs in PyTorch\n",
    "\n",
    "For effiency reasons, PyTorch RNNs require that batches have been sorted by length, with the longest sentence in the batch first. For training, we simply sort each batch. For validation, we would run into trouble if we want to compare our translations with some external file that was not sorted. Therefore we simply set the validation batch size to 1, so that we can keep it in the original order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_data, batch_size=512, train=True, \n",
    "                                 sort_within_batch=True, \n",
    "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False)\n",
    "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False)\n",
    "\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
    "    return Batch(batch.src, batch.trg, pad_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the System\n",
    "\n",
    "Now we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=20, lr=0.0001, print_every=100):\n",
    "    \"\"\"Train a model on sv-en\"\"\"\n",
    "        \n",
    "    # optionally add label smoothing; see the Annotated Transformer\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dev_perplexities = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      \n",
    "        print(\"Epoch\", epoch)\n",
    "        model.train()\n",
    "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
    "                                     model,\n",
    "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
    "                                     print_every=print_every)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
    "\n",
    "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
    "                                       model, \n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "        \n",
    "    return dev_perplexities\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  i you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  i you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  i you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
      "\n",
      "Validation perplexity: 173.500430\n",
      "Epoch 1\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  i &apos;m .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  i &apos;m .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  i &apos;m .\n",
      "\n",
      "Validation perplexity: 84.148539\n",
      "Epoch 2\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  tom is .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  tom is .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  tom is .\n",
      "\n",
      "Validation perplexity: 69.371658\n",
      "Epoch 3\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  tom is a .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he is a .\n",
      "\n",
      "Validation perplexity: 56.805472\n",
      "Epoch 4\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  he is a the .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a the .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he is a the .\n",
      "\n",
      "Validation perplexity: 51.046254\n",
      "Epoch 5\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  he is a .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he is a .\n",
      "\n",
      "Validation perplexity: 43.560969\n",
      "Epoch 6\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  he was a lot .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a lot .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he is a lot .\n",
      "\n",
      "Validation perplexity: 38.052859\n",
      "Epoch 7\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  he was a lot .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was a lot .\n",
      "\n",
      "Validation perplexity: 34.707340\n",
      "Epoch 8\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  the me a lot .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was he was in the .\n",
      "\n",
      "Validation perplexity: 31.354363\n",
      "Epoch 9\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  she me a lot .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was he was in the car .\n",
      "\n",
      "Validation perplexity: 27.979282\n",
      "Epoch 10\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a lot .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was he was in the door .\n",
      "\n",
      "Validation perplexity: 26.074818\n",
      "Epoch 11\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a lot .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was he was .\n",
      "\n",
      "Validation perplexity: 23.302512\n",
      "Epoch 12\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a lot .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was very he .\n",
      "\n",
      "Validation perplexity: 21.618693\n",
      "Epoch 13\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a hat .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was very , but he was .\n",
      "\n",
      "Validation perplexity: 20.022678\n",
      "Epoch 14\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a hat .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good of my father .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was always , he was , he was .\n",
      "\n",
      "Validation perplexity: 19.343409\n",
      "Epoch 15\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a hat .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was very , but , but he was , but , but he was , but .\n",
      "\n",
      "Validation perplexity: 17.975811\n",
      "Epoch 16\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a hat .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was very , but .\n",
      "\n",
      "Validation perplexity: 16.496976\n",
      "Epoch 17\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a hat .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was very , but , but .\n",
      "\n",
      "Validation perplexity: 15.438499\n",
      "Epoch 18\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a hat .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good of my car .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he was going to go .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation perplexity: 15.124592\n",
      "Epoch 19\n",
      "\n",
      "Example #1\n",
      "Src :  ge mig en sekund .\n",
      "Trg :  give me a sec .\n",
      "Pred:  give me a hat .\n",
      "\n",
      "Example #2\n",
      "Src :  det är ett av mina <unk> .\n",
      "Trg :  that &apos;s one of my favorite words .\n",
      "Pred:  it &apos;s a good .\n",
      "\n",
      "Example #3\n",
      "Src :  han gillar grönsaker , <unk> <unk> .\n",
      "Trg :  he likes vegetables , especially <unk> .\n",
      "Pred:  he had to go .\n",
      "\n",
      "Validation perplexity: 14.039815\n",
      "total runing time: 50.14987821488175\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start_t = timeit.default_timer()\n",
    "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
    "                   emb_size=512, hidden_size=512,\n",
    "                   num_layers=2, dropout=0.2)\n",
    "dev_perplexities = train(model, print_every=500)\n",
    "stop_t = timeit.default_timer()\n",
    "print(\"total runing time:\", (stop_t - start_t)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
